{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f529e301-69ce-4919-a92e-c9b3f2930c45",
   "metadata": {},
   "source": [
    "# importing stuff and initial cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75e6ed4-0676-4e20-84d0-01a7a5cc6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import markov_clustering as mc\n",
    "import random\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf70655-94cf-44b4-8c47-8fe8d9e63a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.read_weighted_edgelist('4932.protein.links.v12.0.txt',comments = \"#\",nodetype=str)\n",
    "\n",
    "# remove some edges\n",
    "for u, v in g.edges:\n",
    "    if g.get_edge_data(u, v)['weight'] < 500:\n",
    "      g.remove_edge(u, v)\n",
    "\n",
    "#remove weights\n",
    "for node, edges in nx.to_dict_of_dicts(g).items():\n",
    "    for edge, attrs in edges.items():\n",
    "        attrs.pop('weight', None)\n",
    "\n",
    "matrix = nx.to_numpy_array(g)\n",
    "node_list = list(g.nodes)\n",
    "related_proteins = ['4932.YMR190C','4932.YNL088W','4932.YLR234W','4932.YPL024W','4932.YMR167W' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3837cf72-52aa-4547-9275-e51f7c780073",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f8652f-b323-49fb-b33f-82ab09f4fdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcl(mtx,inflation_parameter):\n",
    "    result = mc.run_mcl(mtx,inflation = inflation_parameter)\n",
    "    clusters = mc.get_clusters(result)\n",
    "\n",
    "    #relabelling node names \n",
    "    for i in range(0,len(clusters)):\n",
    "        clu_list = list(clusters[i])\n",
    "        \n",
    "        for j in range(0,len(clu_list)):\n",
    "            name = node_list[clu_list[j]]\n",
    "            clu_list[j] = name\n",
    "        clusters[i] = tuple(clu_list)\n",
    "\n",
    "    return inflation_parameter, result, clusters   \n",
    "\n",
    "def clu_to_adj_mtx(cluster):\n",
    "    node_index = []\n",
    "    for i in cluster:\n",
    "        node_index.append(node_list.index(i))\n",
    "\n",
    "    mat1 = matrix[node_index, :]\n",
    "    out_mat = mat1[:, node_index]\n",
    "    \n",
    "    return out_mat\n",
    "    \n",
    "def adj_mtx_to_graph(mat,name):\n",
    "    graph = nx.from_numpy_array(mat)\n",
    "    graph = nx.relabel_nodes(graph,name)\n",
    "\n",
    "    return graph\n",
    "\n",
    "def graph_to_cent_meas(graph):\n",
    "    result_dict={}\n",
    "    result_dict['degree'] = sorted(nx.degree_centrality(graph).items(), key=lambda x:x[1],reverse = True)\n",
    "    result_dict['eigenvector'] = sorted(nx.eigenvector_centrality(graph).items(), key=lambda x:x[1],reverse = True)\n",
    "    #result_dict['katz'] = sorted(nx.katz_centrality(graph).items(), key=lambda x:x[1],reverse = True)\n",
    "    result_dict['closeness'] = sorted(nx.closeness_centrality(graph).items(), key=lambda x:x[1],reverse = True)\n",
    "    result_dict['betweenness'] = sorted(nx.betweenness_centrality(graph).items(), key=lambda x:x[1],reverse = True)\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6355b66f-10ac-41d5-a924-eeb1e9f32372",
   "metadata": {},
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a96d30-7765-4541-9a99-79ba7b305d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 is the inflation parameter that maximises modularity \n",
    "inflation_parameter = 1.4\n",
    "\n",
    "markov_clustering_14 = mcl(matrix,1.4)\n",
    "clusters = markov_clustering_14[2]\n",
    "\n",
    "#sort the cluster based on size\n",
    "clusters = sorted(clusters, key=len, reverse=True)\n",
    "\n",
    "modularity = nx.community.modularity(g, clusters, weight='None', resolution=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3819365-b9be-437a-8e50-3192c2030368",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_adj_mtx = []\n",
    "for i in range(0,len(clusters)):\n",
    "    clusters_adj_mtx.append(clu_to_adj_mtx(clusters[i]))\n",
    "\n",
    "clusters_graph = []\n",
    "for i in range(0,len(clusters_adj_mtx)):\n",
    "    clusters_graph.append(adj_mtx_to_graph(clusters_adj_mtx[i]))\n",
    "\n",
    "clusters_cent_meas = []\n",
    "for i in range(0,len(clusters_graph)):\n",
    "    clusters_cent_meas.append(graph_to_cent_meas(clusters_graph[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dc5bb0-334a-4046-b3b5-b5189bb69902",
   "metadata": {},
   "source": [
    "# sgs1 cluster and its centrality measure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f195267-be6b-4f76-a4d1-bfc888cec0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking which cluster SGS1 and other related proteins are in \n",
    "for i in clusters:\n",
    "    if '4932.YMR190C' in i: \n",
    "        sgs1_clu_index = clusters.index(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885b874b-6df4-4d29-8833-1d7de61576ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking specifically at SGS1 cluster\n",
    "\n",
    "#number of nodes \n",
    "print(len(clusters[sgs1_clu_index]))\n",
    "\n",
    "#drawing them \n",
    "nx.draw(clusters_graph[sgs1_clu_index])\n",
    "\n",
    "#centrality measure for each node in the cluster\n",
    "clusters_cent_meas[sgs1_clu_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad80cdb-2e86-40c3-b498-06e06d1a4d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#centrality measure for sgs1 and the gang\n",
    "for i in clusters_cent_meas[sgs1_clu_index]\n",
    "    print(i)\n",
    "    for j in related_proteins:\n",
    "        print(j,clusters_cent_meas[sgs1_clu_index][i].index(j))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8088e22a-bf93-45a9-8b3f-0252c9673f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 6 nodes sorted by centrality\n",
    "for i in clusters_cent_meas[sgs1_clu_index]:\n",
    "    print(i)\n",
    "    for j in range(0,6):\n",
    "        print(clusters_cent_meas[sgs1_clu_index][i][j])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de48cdc1-484d-4a8f-b267-93a03e0a65ce",
   "metadata": {},
   "source": [
    "# other clusters and their \"important\" nodes\n",
    "\n",
    "I think it is kind of pointless and annoying to look at small communities & singletons, which is why they'll be removed for the following instances. In particular, any community with less than 20 nodes. We then take the top few nodes for each centrality measure for each community. I think it might be worthwhile to look at them based on community size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc434ec-f231-4d12-b7c6-3fb290f505c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove small clusters\n",
    "large_clusters_cent_meas=[]\n",
    "for i in clusters_cent_meas:\n",
    "    if len(i['degree'])>=20:\n",
    "        large_clusters_cent_meas.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716dc938-d52e-4a82-931d-3d925b625547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints the first 6 nodes in list\n",
    "for i in large_clusters_cent_meas:\n",
    "    for j in i: \n",
    "        print(j)\n",
    "        for k in range(0,6)\n",
    "            print(i[j][k])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca54a80-97c0-4292-b315-d55894bd08dc",
   "metadata": {},
   "source": [
    "# comparison with clustering using other algorithm\n",
    "\n",
    "Import pickle or json file, then calculate the maximum similarity between the two communities. My idea is that we count how many nodes are different and normalise based on number of nodes in a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fbbefa-ebb6-4156-8a87-224a1268b0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
