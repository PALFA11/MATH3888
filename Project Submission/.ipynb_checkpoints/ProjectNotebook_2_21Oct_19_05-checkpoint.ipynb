{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Packages to be used \n",
    "\n",
    "# Network Stuff \n",
    "import networkx as nx\n",
    "import markov_clustering as mc\n",
    "import random\n",
    "import igraph \n",
    "from gprofiler import GProfiler\n",
    "p = GProfiler(return_dataframe=True)\n",
    "\n",
    "\n",
    "# Standard \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "import math\n",
    "\n",
    "\n",
    "# %matplotlib inline \n",
    "# font = {'family' : 'DejaVu Sans',\n",
    "#         'weight' : 'bold',\n",
    "#         'size'   : 32}\n",
    "\n",
    "# plt.rc('font', **font)\n",
    "\n",
    "\n",
    "g = nx.read_weighted_edgelist(\"C:\\\\Users\\\\yuanl\\\\OneDrive\\\\Documents\\\\Professional\\\\Projects\\\\MATH3888 - Biological Modelling\\\\Data\\\\4932.protein.links.v12.0.txt\",comments=\"#\",nodetype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u, v in g.edges:\n",
    "    if g.get_edge_data(u, v)['weight'] < 500:\n",
    "      g.remove_edge(u, v)\n",
    "\n",
    "#remove weights\n",
    "for node, edges in nx.to_dict_of_dicts(g).items():\n",
    "    for edge, attrs in edges.items():\n",
    "        attrs.pop('weight', None)\n",
    "\n",
    "matrix = nx.to_numpy_array(g)\n",
    "node_list = list(g.nodes)\n",
    "\n",
    "# These are the important connections that we know SGS1 connects to \n",
    "related_proteins = ['4932.YMR190C','4932.YNL088W','4932.YLR234W','4932.YPL024W','4932.YMR167W' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### Functions to be created/copypasted \n",
    "\n",
    "# def graph_details() \n",
    "    # Number of nodes\n",
    "    # Number of edges \n",
    "    # Connectivity Status (Fully or Not) \n",
    "    # Degree Distribution: PLOTTING \n",
    "    # Degree of SGS1 Node \n",
    "    # .... \n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# markov clustering, with inflation parameter\n",
    "def mcl(mtx,inflation_parameter):\n",
    "    result = mc.run_mcl(mtx,inflation = inflation_parameter)\n",
    "    clusters = mc.get_clusters(result)\n",
    "\n",
    "    #relabelling node names \n",
    "    for i in range(0,len(clusters)):\n",
    "        clu_list = list(clusters[i])\n",
    "        \n",
    "        for j in range(0,len(clu_list)):\n",
    "            name = node_list[clu_list[j]]\n",
    "            clu_list[j] = name\n",
    "        clusters[i] = tuple(clu_list)\n",
    "\n",
    "    return inflation_parameter, result, clusters   \n",
    "\n",
    "def large_comm(cluster,threshold):\n",
    "    large = []\n",
    "    for i in cluster:\n",
    "        if len(i)>=threshold:\n",
    "            large.append(i)\n",
    "\n",
    "    return large\n",
    "\n",
    "#turns clusters into adjacency matrix\n",
    "def clu_to_adj_mtx(cluster):\n",
    "    node_index = []\n",
    "    for i in cluster:\n",
    "        node_index.append(node_list.index(i))\n",
    "\n",
    "    mat1 = matrix[node_index, :]\n",
    "    out_mat = mat1[:, node_index]\n",
    "    \n",
    "    return out_mat\n",
    "\n",
    "#turns adjacency matrix into graph \n",
    "def adj_mtx_to_graph(mat,name):\n",
    "    graph = nx.from_numpy_array(mat)\n",
    "    graph = nx.relabel_nodes(graph,name)\n",
    "\n",
    "    return graph\n",
    "\n",
    "# turns graph into outputs from some centrality measures, \n",
    "# The centrality measures include degree centrality, eigenvector centrality, closeness centrality, and betweenness centrality\n",
    "def graph_to_cent_meas(graph):\n",
    "    result_dict={}\n",
    "    result_dict['degree'] = sorted(nx.degree_centrality(graph).items(), key=lambda x:x[1],reverse = True)\n",
    "    result_dict['eigenvector'] = sorted(nx.eigenvector_centrality(graph).items(), key=lambda x:x[1],reverse = True)\n",
    "    result_dict['closeness'] = sorted(nx.closeness_centrality(graph).items(), key=lambda x:x[1],reverse = True)\n",
    "    result_dict['betweenness'] = sorted(nx.betweenness_centrality(graph).items(), key=lambda x:x[1],reverse = True)\n",
    "\n",
    "def important_nodes(cent_meas_of_clus, n_of_nodes):\n",
    "    for i in cent_meas_of_clus:\n",
    "        print(i+':')\n",
    "        for j in range(0,n_of_nodes):\n",
    "            print(cent_meas_of_clus[i][j])\n",
    "        print('\\n')\n",
    "\n",
    "    # COMMUNITY FINDING ALGOS HERE \n",
    "\n",
    "    # def community_partitions(graph, initial_thresh, trials) \n",
    "    # Edge Dropout \n",
    "    # threshold_score = initial_thresh\n",
    "    # for edge in graph.edges: \n",
    "    #     weight = list(graph.get_edge_data(edge[0],edge[1]).values())\n",
    "    #     if(weight[0] < threshold_score):\n",
    "    #         graph.remove_edge(edge[0],edge[1])\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Markov Clustering\n",
    "\n",
    "# 1.4 is the inflation parameter that maximises modularity \n",
    "inflation_parameter = 1.4\n",
    "markov_clustering_14 = mcl(matrix,inflation_parameter)\n",
    "\n",
    "#sort the cluster based on size\n",
    "clusters = markov_clustering_14[2]\n",
    "clusters = sorted(clusters, key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matches number to the name of the node \n",
    "cluster_nodename_dict_list=[]\n",
    "for i in clusters: \n",
    "    clu_dict = {}\n",
    "    for j in range(0,len(i)):\n",
    "        clu_dict[j] = i[j]\n",
    "\n",
    "    cluster_nodename_dict_list.append(clu_dict)\n",
    "\n",
    "#the following turns clusters into adjacency matrices into graphs into centrality measures for each clusters \n",
    "clusters_adj_mtx = []\n",
    "for i in range(0,len(clusters)):\n",
    "    clusters_adj_mtx.append(clu_to_adj_mtx(clusters[i]))\n",
    "\n",
    "clusters_graph = []\n",
    "for i in range(0,len(clusters_adj_mtx)):\n",
    "    clusters_graph.append(adj_mtx_to_graph(clusters_adj_mtx[i],cluster_nodename_dict_list[i]))\n",
    "\n",
    "clusters_cent_meas = []\n",
    "for i in range(0,len(clusters_graph)):\n",
    "    clusters_cent_meas.append(graph_to_cent_meas(clusters_graph[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centrality measures \n",
    "\n",
    "# Only look at larger clusters \n",
    "large_clusters_cent_meas=[]\n",
    "for i in range(0,len(clusters_cent_meas)):\n",
    "    if len(clusters_cent_meas[i]['degree'])>=20:\n",
    "        large_clusters_cent_meas.append(clusters_cent_meas[i])\n",
    "\n",
    "nodes_to_print = 10\n",
    "counter = 0\n",
    "for i in large_clusters_cent_meas:\n",
    "    print(counter,'~~~~~~~~~~~~~~~~~~~~')\n",
    "    important_nodes(i,nodes_to_print)\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put code for fast label prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison with other algorithms \n",
    "def similarity_of_clusterings(clusters1,name1, clusters2,name2):\n",
    "    clusters1 = sorted(clusters1,key = len, reverse = True)\n",
    "    clusters2 = sorted(clusters2,key = len, reverse = True)\n",
    "\n",
    "    sim_list = []\n",
    "    for i in range(0,len(clusters1)) :\n",
    "        similarity = []\n",
    "        for j in range(0,len(clusters2)):\n",
    "            counter = 0\n",
    "            for k in clusters1[i]:\n",
    "                if k in clusters2[j]:\n",
    "                    counter +=1\n",
    "            score = 2*counter/(len(clusters1[i])+len(clusters2[j]))\n",
    "            if score > 0:\n",
    "                similarity.append((str(name2)+str(j)+' '+str(len(clusters2[j]))+':',score))\n",
    "        similarity = sorted(similarity, key = lambda x:x[1], reverse= True)\n",
    "        similarity.insert(0,str(str(name1)+str(i)+' '+str(len(clusters1[i]))+':'))\n",
    "        sim_list.append(similarity)\n",
    "    return sim_list\n",
    "\n",
    "sim_llp_lmc = similarity_of_clusterings(large_comm(label_prop_1,30),'lp',large_comm(clusters,30),'mc')\n",
    "for i in sim_llp_lmc:\n",
    "    print(i[0],i[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Algorithms \n",
    "\n",
    "\n",
    "\n",
    "    # VERIFICATION STEPS \n",
    "    # Check if there are singletons: WHAT TO DO WITH THESE SINGLETONS? \n",
    "    # Check ????? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # RELEVANT OUTPUTS \n",
    "\n",
    "    # nodeassignment = \n",
    "    # Label prop and markov clustering have their output as list of list/tuples where \n",
    "    # the first layer is the list of communities and second layer is the list/tuple of the nodes within each community\n",
    "\n",
    "    # nodenames  = list(nxgraph.nodes()) #NAMES VECTOR \n",
    "    # commnum = max(value for _, value in enumerate(nodeassignment))+1 #THIS SHOULD WORK IF NODEASSIGNMENT WORKS \n",
    "    # commdict = {} #A DICTIONARY WHOSE KEY IS THE COMMUNITY AND VALUE WILL BE A LIST  OF CLUSTERS IN THAT DICT  \n",
    "\n",
    "\n",
    "    # for i in range(commnum): \n",
    "    # commdict[i] = []\n",
    "    # for nodeint, cluster in enumerate(nodeassignment): \n",
    "    #     nodename = nodenames[nodeint]\n",
    "    #     commdict[cluster].append(nodename) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # return nodenames, nodeassignment, commdict, commnum\n",
    "\n",
    "\n",
    "\n",
    "# def overlapping_algo(PARAMS HERE) #THIS IS OUR MAIN ALGORITHM  \n",
    "    \n",
    "    \n",
    "    # ANCHORS \n",
    "    # Run an initial community finding trial\n",
    "    # Determine anchors \n",
    "\n",
    "\n",
    "    # DRIFTERS\n",
    "    # SGS1connections = [\"YMR190C\", \"YNL088W\", \"YLR234W\", \"YPL024W\", \"YMR167W\"] # These are the important connections that we know SGS1 connects to \n",
    "    # testings = gp.profile( organism=\"scerevisiae\", query=[\"YMR190C\", \"YNL088W\", \"YLR234W\", \"YPL024W\", \"YMR167W\"])\n",
    "\n",
    "\n",
    "    # PATH CONSTRUCTION \n",
    "    # Use profilers to eliminate irrelevant ones\n",
    "    # Calculate ratio and define a ratio as \"good\" such that we consider it an appropriate path to go down\n",
    "    # Determine the path to go down based on ratio which can be considered as edges of a graph\n",
    "    # Repeat Step 5 to continue constructing edges\n",
    "    # If a previously visited community/\"drifter\" or a dead end is reached, choose another path with good ratio to go down or backtrack to previous nodes with appropriate paths\n",
    "    # Explore all appropriate paths???? \n",
    "\n",
    "    return #A FINAL LIST OF THE NODES THAT DENOTE A PATH STARTING FROM SGS1 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def visualization_graph() \n",
    "    # Output the resulting graph: need to get a list? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodological Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### Visualization of Graphs \n",
    "\n",
    "## PUT THE VISUALIZATION OUTPUTS HERE "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
