{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Packages to be used \n",
    "\n",
    "# Network Stuff \n",
    "import networkx as nx\n",
    "import markov_clustering as mc\n",
    "import random\n",
    "import igraph \n",
    "from gprofiler import GProfiler\n",
    "p = GProfiler(return_dataframe=True)\n",
    "\n",
    "\n",
    "# Standard \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "import math\n",
    "\n",
    "\n",
    "# %matplotlib inline \n",
    "# font = {'family' : 'DejaVu Sans',\n",
    "#         'weight' : 'bold',\n",
    "#         'size'   : 32}\n",
    "\n",
    "# plt.rc('font', **font)\n",
    "\n",
    "\n",
    "g = nx.read_weighted_edgelist(\"C:\\\\Users\\\\yuanl\\\\OneDrive\\\\Documents\\\\Professional\\\\Projects\\\\MATH3888 - Biological Modelling\\\\Data\\\\4932.protein.links.v12.0.txt\",comments=\"#\",nodetype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u, v in g.edges:\n",
    "    if g.get_edge_data(u, v)['weight'] < 500:\n",
    "      g.remove_edge(u, v)\n",
    "\n",
    "#remove weights\n",
    "for node, edges in nx.to_dict_of_dicts(g).items():\n",
    "    for edge, attrs in edges.items():\n",
    "        attrs.pop('weight', None)\n",
    "\n",
    "matrix = nx.to_numpy_array(g)\n",
    "node_list = list(g.nodes)\n",
    "\n",
    "# These are the important connections that we know SGS1 connects to \n",
    "related_proteins = ['4932.YMR190C','4932.YNL088W','4932.YLR234W','4932.YPL024W','4932.YMR167W' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### Functions to be created/copypasted \n",
    "\n",
    "# def graph_details() \n",
    "    # Number of nodes\n",
    "    # Number of edges \n",
    "    # Connectivity Status (Fully or Not) \n",
    "    # Degree Distribution: PLOTTING \n",
    "    # Degree of SGS1 Node \n",
    "    # .... \n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# markov clustering, with inflation parameter\n",
    "def mcl(mtx,inflation_parameter):\n",
    "    result = mc.run_mcl(mtx,inflation = inflation_parameter)\n",
    "    clusters = mc.get_clusters(result)\n",
    "\n",
    "    #relabelling node names \n",
    "    for i in range(0,len(clusters)):\n",
    "        clu_list = list(clusters[i])\n",
    "        \n",
    "        for j in range(0,len(clu_list)):\n",
    "            name = node_list[clu_list[j]]\n",
    "            clu_list[j] = name\n",
    "        clusters[i] = tuple(clu_list)\n",
    "\n",
    "    return inflation_parameter, result, clusters   \n",
    "\n",
    "def large_comm(cluster,threshold):\n",
    "    large = []\n",
    "    for i in cluster:\n",
    "        if len(i)>=threshold:\n",
    "            large.append(i)\n",
    "\n",
    "    return large\n",
    "\n",
    "#turns clusters into adjacency matrix\n",
    "def clu_to_adj_mtx(cluster):\n",
    "    node_index = []\n",
    "    for i in cluster:\n",
    "        node_index.append(node_list.index(i))\n",
    "\n",
    "    mat1 = matrix[node_index, :]\n",
    "    out_mat = mat1[:, node_index]\n",
    "    \n",
    "    return out_mat\n",
    "\n",
    "#turns adjacency matrix into graph \n",
    "def adj_mtx_to_graph(mat,name):\n",
    "    graph = nx.from_numpy_array(mat)\n",
    "    graph = nx.relabel_nodes(graph,name)\n",
    "\n",
    "    return graph\n",
    "\n",
    "# turns graph into outputs from some centrality measures, \n",
    "# The centrality measures include degree centrality, eigenvector centrality, closeness centrality, and betweenness centrality\n",
    "def graph_to_cent_meas(graph):\n",
    "    result_dict={}\n",
    "    result_dict['degree'] = sorted(nx.degree_centrality(graph).items(), key=lambda x:x[1],reverse = True)\n",
    "    result_dict['eigenvector'] = sorted(nx.eigenvector_centrality(graph).items(), key=lambda x:x[1],reverse = True)\n",
    "    result_dict['closeness'] = sorted(nx.closeness_centrality(graph).items(), key=lambda x:x[1],reverse = True)\n",
    "    result_dict['betweenness'] = sorted(nx.betweenness_centrality(graph).items(), key=lambda x:x[1],reverse = True)\n",
    "\n",
    "def important_nodes(cent_meas_of_clus, n_of_nodes):\n",
    "    for i in cent_meas_of_clus:\n",
    "        print(i+':')\n",
    "        for j in range(0,n_of_nodes):\n",
    "            print(cent_meas_of_clus[i][j])\n",
    "        print('\\n')\n",
    "\n",
    "    # COMMUNITY FINDING ALGOS HERE \n",
    "\n",
    "    # def community_partitions(graph, initial_thresh, trials) \n",
    "    # Edge Dropout \n",
    "    # threshold_score = initial_thresh\n",
    "    # for edge in graph.edges: \n",
    "    #     weight = list(graph.get_edge_data(edge[0],edge[1]).values())\n",
    "    #     if(weight[0] < threshold_score):\n",
    "    #         graph.remove_edge(edge[0],edge[1])\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Markov Clustering\n",
    "\n",
    "# 1.4 is the inflation parameter that maximises modularity \n",
    "inflation_parameter = 1.4\n",
    "markov_clustering_14 = mcl(matrix,inflation_parameter)\n",
    "\n",
    "#sort the cluster based on size\n",
    "clusters = markov_clustering_14[2]\n",
    "clusters = sorted(clusters, key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matches number to the name of the node \n",
    "cluster_nodename_dict_list=[]\n",
    "for i in clusters: \n",
    "    clu_dict = {}\n",
    "    for j in range(0,len(i)):\n",
    "        clu_dict[j] = i[j]\n",
    "\n",
    "    cluster_nodename_dict_list.append(clu_dict)\n",
    "\n",
    "#the following turns clusters into adjacency matrices into graphs into centrality measures for each clusters \n",
    "clusters_adj_mtx = []\n",
    "for i in range(0,len(clusters)):\n",
    "    clusters_adj_mtx.append(clu_to_adj_mtx(clusters[i]))\n",
    "\n",
    "clusters_graph = []\n",
    "for i in range(0,len(clusters_adj_mtx)):\n",
    "    clusters_graph.append(adj_mtx_to_graph(clusters_adj_mtx[i],cluster_nodename_dict_list[i]))\n",
    "\n",
    "clusters_cent_meas = []\n",
    "for i in range(0,len(clusters_graph)):\n",
    "    clusters_cent_meas.append(graph_to_cent_meas(clusters_graph[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centrality measures \n",
    "\n",
    "# Only look at larger clusters \n",
    "large_clusters_cent_meas=[]\n",
    "for i in range(0,len(clusters_cent_meas)):\n",
    "    if len(clusters_cent_meas[i]['degree'])>=20:\n",
    "        large_clusters_cent_meas.append(clusters_cent_meas[i])\n",
    "\n",
    "nodes_to_print = 10\n",
    "counter = 0\n",
    "for i in large_clusters_cent_meas:\n",
    "    print(counter,'~~~~~~~~~~~~~~~~~~~~')\n",
    "    important_nodes(i,nodes_to_print)\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################## \n",
    "############### put code for fast label prop \n",
    "#########################\n",
    "\n",
    "\n",
    "\n",
    "### INPUT : Gc   <--- a connected Graph \n",
    "\n",
    "\n",
    "###################################################################################################### \n",
    "commus = nx.community.label_propagation_communities(Gc) \n",
    "print(\"commu # :\" , len(commus))\n",
    "modularity_v1 = nx.community.modularity(Gc , commus) \n",
    "\n",
    "print(\"modularity :\" , modularity_v1) \n",
    "\n",
    "\n",
    "G = nx.Graph(Gc) \n",
    "for i in commus : \n",
    "    if len(i) < 2 : \n",
    "        for ii in i : \n",
    "            G.remove_node(ii) \n",
    "print(\"////////// REMOVAL ///////////\") \n",
    "print(\"\\t|G0| :\" , G0.number_of_nodes()) \n",
    "print(\"\\t|Gc| :\" , Gc.number_of_nodes()) \n",
    "print(\"\\t|G| :\" , G.number_of_nodes()) \n",
    "\n",
    "newer_commus = nx.community.label_propagation_communities(G) \n",
    "modularity_v2 = nx.community.modularity(G , newer_commus) \n",
    "\n",
    "print(\"new modularity :\" , modularity_v2) \n",
    "#######################################################################################  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################### \n",
    "###################################################################################################### \n",
    "def run_many_times(n=101 , G=G) : \n",
    "    run_records = [] \n",
    "    for i in range(n) : \n",
    "        random_number_in_range = random.randint(0 , 2**(64)-1) \n",
    "        rv = nx.community.fast_label_propagation_communities(G , seed=random_number_in_range) \n",
    "#        rv = nx.community.asyn_fluidc(G , 80 , seed=random_number_in_range) \n",
    "#        rv = nx.community.louvain_communities(G , seed=random_number_in_range) \n",
    "#        rv = nx.community.kernighan_lin_bisection(G , seed=random_number_in_range) \n",
    "        rv_list = [] \n",
    "        \n",
    "        \n",
    "        for ii in rv : \n",
    "            rv_list.append(ii) \n",
    "        #for i in rv_list : \n",
    "        #    print(i) \n",
    "        \n",
    "        run_records.append([rv_list , random_number_in_range]) \n",
    "        \n",
    "        print(\"                    \" , end='\\r') \n",
    "        print(\"( O v O) {\" , i+1 , \")\" , end='\\r') \n",
    "\n",
    "    \n",
    "    return run_records \n",
    "###################################################################################################### \n",
    "###################################################################################################### \n",
    "def tmp_F(runs , in_G=G) :  \n",
    "    rela_c = {} \n",
    "    for i in in_G : \n",
    "        rela_c[i] = {} \n",
    "        for ii in in_G : \n",
    "            rela_c[i][ii] = 0 \n",
    "\n",
    "    tmp_c = 0 \n",
    "    for iiii in runs : \n",
    "        for i in in_G : \n",
    "            for ii in iiii[0] : \n",
    "                if i in ii : \n",
    "                    for iii in ii : \n",
    "                        if i == iii : \n",
    "                            continue \n",
    "                        rela_c[i][iii] += 1 \n",
    "        tmp_c += 1 \n",
    "        print(str(tmp_c) + \"/\" + str(len(runs)) + \" RUN\\t[ OK ]\" , end='\\r') \n",
    "\n",
    "    return rela_c \n",
    "###################################################################################################### \n",
    "###################################################################################################### \n",
    "def dense_commu_of(tar_node , rela_counter_in , run_result) : \n",
    "    dense_commu = [] \n",
    "    drifters = [] \n",
    "    for i in rela_counter_in[tar_node] : \n",
    "        if rela_counter_in[tar_node][i]/len(run_result) >= 0.95 :        #overlap 95% of times \n",
    "            dense_commu.append(i) \n",
    "        elif rela_counter_in[tar_node][i]/len(run_result) >= 0.5 : \n",
    "            if i != tar_node : \n",
    "                drifters.append(i) \n",
    "            \n",
    "    return [dense_commu , drifters] \n",
    "###################################################################################################### \n",
    "###################################################################################################### \n",
    "def QC_Idea_v2(relas , runs , starting_node=\"4932.YMR190C\") : \n",
    "    \n",
    "    self_def_commus = [] \n",
    "    \n",
    "    all_nodes = [] \n",
    "    all_nodes_info = {} \n",
    "    for i in relas : \n",
    "        all_nodes.append( i ) \n",
    "        all_nodes_info[i] = [0 , -1 , []]    # [?dense , dense # , belong] \n",
    "    #print(\">>\" , len(all_nodes_info)) \n",
    "\n",
    "    new_commu_id_counter = 0 \n",
    "\n",
    "    if len(starting_node) > 0 : \n",
    "        cur_tar = starting_node \n",
    "    \n",
    "\n",
    "        raw_dense_list , raw_drifter_list = dense_commu_of(cur_tar , relas , runs) \n",
    "\n",
    "        new_commu_pack = [cur_tar] \n",
    "        for ii in raw_dense_list : \n",
    "            #if 0 == all_nodes_info[ii][0] : \n",
    "            new_commu_pack.append(ii) \n",
    "        for ii in new_commu_pack : \n",
    "            all_nodes_info[ii][0] = 1 \n",
    "            all_nodes_info[ii][1] = new_commu_id_counter \n",
    "            all_nodes_info[ii][2].append(new_commu_id_counter) \n",
    "        for ii in raw_drifter_list : \n",
    "            all_nodes_info[ii][2].append(new_commu_id_counter) \n",
    "    \n",
    "        self_def_commus.append(new_commu_pack) \n",
    "    \n",
    "    \n",
    "        new_commu_id_counter = 1 \n",
    "\n",
    "\n",
    "    for i in relas : \n",
    "        cur_tar = i \n",
    "        if all_nodes_info[cur_tar][0] == 1 :      # already dense \n",
    "            continue \n",
    "#        elif len(all_nodes_info[cur_tar][2]) > 0 :    # drifter \n",
    "#            continue \n",
    "        else : \n",
    "            tmp = dense_commu_of(cur_tar , relas , runs) \n",
    "            raw_dense_list = tmp[0] \n",
    "            raw_drifter_list = tmp[1] \n",
    "            new_commu_pack = [cur_tar] \n",
    "\n",
    "            if len(raw_dense_list) < 11 : \n",
    "                continue \n",
    "            \n",
    "            for ii in raw_dense_list : \n",
    "                if 0 == all_nodes_info[ii][0] : \n",
    "                    new_commu_pack.append(ii) \n",
    "                    \n",
    "            if len(new_commu_pack) > 10 : \n",
    "                self_def_commus.append(new_commu_pack) \n",
    "            else : \n",
    "                continue \n",
    "                \n",
    "            for ii in new_commu_pack : \n",
    "                all_nodes_info[ii][0] = 1 \n",
    "                all_nodes_info[ii][1] = new_commu_id_counter \n",
    "                all_nodes_info[ii][2].append(new_commu_id_counter) \n",
    "            for ii in raw_drifter_list : \n",
    "                all_nodes_info[ii][2].append(new_commu_id_counter) \n",
    "\n",
    "\n",
    "        new_commu_id_counter += 1 \n",
    "\n",
    "            \n",
    "        \n",
    "    return [self_def_commus , all_nodes_info] \n",
    "###################################################################################################### \n",
    "###################################################################################################### \n",
    "def possi_dests(ni , dense_commus , nodes_info) : \n",
    "\n",
    "    trig = 0 \n",
    "    \n",
    "    ni_idx = 0 \n",
    "    for i in dense_commus : \n",
    "        if ni in i : \n",
    "            print(\"[\" + str(ni) + \"]\" , \"@\" , ni_idx)\n",
    "            break \n",
    "        ni_idx += 1 \n",
    "\n",
    "\n",
    "    possi_padding = [] \n",
    "    possi_dest = [] \n",
    "    feelted_possi_dest = [] \n",
    "    feelted_possi_padding = [] \n",
    "\n",
    "    \n",
    "    for i in nodes_info : \n",
    "        if (ni_idx in nodes_info[i][2]) and nodes_info[i][0] == 0 :    #nodes_info[i][0] == 0 and \n",
    "            # collect drifters \n",
    "            possi_padding.append(i) \n",
    "            for ii in nodes_info[i][2] : \n",
    "                if ii not in possi_dest and ii != ni_idx : \n",
    "                    possi_dest.append(ii) \n",
    "    #print(len(possi_padding) , possi_dest) \n",
    "    #print(len(dense_commus)) \n",
    "    #print(\"~~~~~~~~~~~~\") \n",
    "    for i in possi_dest : ########## \n",
    "        \n",
    "        if len(dense_commus[i]) >= 2 : \n",
    "            feelted_possi_dest.append(i) \n",
    "    if 0 == len(feelted_possi_dest) : \n",
    "        print(\"( ' ^ ') { EMPTY )\") \n",
    "        trig = 1 \n",
    "\n",
    "    #print(feelted_possi_dest) \n",
    "    #print(\"paddings :\" , str(len(possi_padding)) + \"<>\" + str(len(tmp_box[ni_idx])) , \": dense\") \n",
    "    if trig : \n",
    "        return [possi_dest , possi_padding] \n",
    "    return [feelted_possi_dest , possi_padding] \n",
    "###################################################################################################### \n",
    "###################################################################################################### \n",
    "def tmp_H(starting_node , to_commus , drifters , node_infos , dense_commus , G=G): \n",
    "  #                      other_sides   paddings   all_node_info   self_def_commus \n",
    "    idx = 0 \n",
    "    for i in dense_commus : \n",
    "        if starting_node in i : \n",
    "            break \n",
    "        idx += 1 \n",
    "\n",
    "    for i in to_commus : \n",
    "        tmp_pack = [] \n",
    "        dd_list = [] \n",
    "        tmp_G = G.subgraph(dense_commus[i]) \n",
    "        \n",
    "    \n",
    "        for ii in drifters : \n",
    "            if i in node_infos[ii][2] : \n",
    "                tmp_pack.append(ii) \n",
    "        tmp_top_btw = top_few_btween_member(tmp_G , \"\" , 0) \n",
    "        print(\"   starting point\" , \":\\t\" , starting_node , \" @ \" , idx) \n",
    "        #print(\"\\tdrifters :\\t\" , tmp_pack) \n",
    "        print(\"essens 0f commu_\" + str(i) + \" :\\t\" , tmp_top_btw) \n",
    "\n",
    "\n",
    "\n",
    "        ################## \n",
    "        jump_path = [] \n",
    "        land_path = [] \n",
    "        ################## \n",
    "        for ii in dense_commus[idx] : \n",
    "            #dd_list.append(ii) \n",
    "            jump_path.append(ii) \n",
    "        for ii in tmp_pack : \n",
    "            dd_list.append(ii) \n",
    "            jump_path.append(ii) \n",
    "            land_path.append(ii) \n",
    "        for ii in dense_commus[i] : \n",
    "            #dd_list.append(ii) \n",
    "            land_path.append(ii) \n",
    "        dd_G = G.subgraph(dd_list) \n",
    "        tmp_top_btw_drifters = top_few_btween_member(dd_G , \"\" , 0) \n",
    "\n",
    "        ########## \n",
    "        new_tmp_pack = [] \n",
    "        for ii in tmp_top_btw_drifters : \n",
    "            new_tmp_pack.append(ii) \n",
    "        tmp_pack = new_tmp_pack \n",
    "        print(\"essen drifer <\" , idx , i , \">:\" , tmp_pack , \"\\n\") \n",
    "        ########\n",
    "    \n",
    "        for ii in range(len(tmp_top_btw)) : \n",
    "            try : \n",
    "\n",
    "                shortest_path_for_now = [] \n",
    "                via_node = \"[ NOTHING ]\" \n",
    "                \n",
    "                for iii in range(len(tmp_pack)) : \n",
    "                ########## \n",
    "                    try : \n",
    "                        G_LR = G.subgraph(jump_path) \n",
    "                        left_p = nx.shortest_path(G_LR , starting_node , tmp_pack[iii]) \n",
    "                        G_LR = G.subgraph(land_path) \n",
    "                        right_p = nx.shortest_path(G_LR , tmp_pack[iii] , tmp_top_btw[ii]) \n",
    "                        if iii == 0 : \n",
    "                            shortest_path_for_now = left_p[:-1] + right_p \n",
    "                            via_node = tmp_pack[iii] \n",
    "                            continue \n",
    "                        if len( left_p[:-1] + right_p ) < len(shortest_path_for_now) : \n",
    "                            shortest_path_for_now = left_p[:-1] + right_p \n",
    "                            via_node = tmp_pack[iii] \n",
    "                    except nx.NetworkXNoPath : \n",
    "                        continue \n",
    "\n",
    "                ########## \n",
    "                #print(\">>>LinkingPath \" + str(ii+1) + \" : HEAD --> TAIL\\n\\t\\t\" , nx.shortest_path(dd_G , starting_node , tmp_top_btw[ii]) ) \n",
    "                if 0 == len(shortest_path_for_now) : \n",
    "                    print(\">>>LinkingPath \" + str(ii+1) + \" : HEAD --> TAIL  via\" , via_node , \"\\n\\t\\t\" , \"[ NO PATH ]\" ) \n",
    "                    continue \n",
    "                print(\">>>LinkingPath \" + str(ii+1) + \" : HEAD --> TAIL  via\" , via_node , \"\\n\\t\\t\" , shortest_path_for_now ) \n",
    "                #break \n",
    "            except nx.NetworkXNoPath : \n",
    "                print(\"can't reach\" , \"[\" + tmp_top_btw[ii] + \"]\") \n",
    "                if ii == len(tmp_top_btw) - 1 : \n",
    "                    print(\"[ NO PATH ]\") \n",
    "                #print(\"/////////////////////////////////////////\\n////////////// NO PATH /////////////////\\n///////////////////////////////////\") \n",
    "                \n",
    "            \n",
    "        #print(\">>>LinkingPath : HEAD --> Drifter\\n\\t\\t\" , nx.shortest_path(G , \"4932.YMR190C\" , \"4932.YDR545W\") ) \n",
    "            \n",
    "        print(\"\\n\\n\\n\") \n",
    "###################################################################################################### \n",
    "###################################################################################################### \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tar_node = \"4932.YMR190C\" \n",
    "\n",
    "###################################################################################################### \n",
    "run_result = run_many_times(33 , G) \n",
    "rela_counter = tmp_F(run_result , G) \n",
    "###################################################################################################### \n",
    "tmp = QC_Idea_v2(rela_counter , run_result , tar_node) \n",
    "self_def_commus = tmp[0]    #self def commus \n",
    "all_node_info = tmp[1]     # info \n",
    "tar_idx = 0 \n",
    "for i in self_def_commus : \n",
    "    if tar_node in i : \n",
    "        print(\"[\" + tar_node + \"]\" , \"@\" , tar_idx) \n",
    "        break \n",
    "    tar_idx += 1 \n",
    "###################################################################################################### \n",
    "print(len(self_def_commus)) \n",
    "tmp_l = [] \n",
    "for i in all_node_info :\n",
    "    for ii in all_node_info[i][2] : \n",
    "        tmp_l.append(ii) \n",
    "print(max(tmp_l)) \n",
    "###################################################################################################### \n",
    "feelted_self_def_commus = [] \n",
    "for i in self_def_commus : \n",
    "    if len(i) > 7 : \n",
    "        feelted_self_def_commus.append(i) \n",
    "\n",
    "print(\" |pre feelter| =\" , len(self_def_commus)) \n",
    "print(\"|post feelter| =\" , len(feelted_self_def_commus)) \n",
    "###################################################################################################### \n",
    "other_sides , paddings = possi_dests(tar_node , self_def_commus , all_node_info) \n",
    "\n",
    "print(other_sides) \n",
    "print(len(paddings)) \n",
    "###################################################################################################### \n",
    "\n",
    "\"\"\" \n",
    "print() \n",
    "for i in other_sides : \n",
    "    tmp_pack = [] \n",
    "    dd_list = [] \n",
    "    tmp_G = G.subgraph(self_def_commus[i]) \n",
    "    \n",
    "\n",
    "    for ii in paddings : \n",
    "        if i in all_node_info[ii][2] : \n",
    "            tmp_pack.append(ii) \n",
    "    tmp_top_btw = top_few_btween_member(tmp_G , \"\" , 0) \n",
    "    print(\"   starting point\" , \":\\t\" , \"4932.YMR190C\") \n",
    "    #print(\"\\tdrifters :\\t\" , tmp_pack) \n",
    "    print(\"essens 0f commu_\" + str(i) + \" :\\t\" , tmp_top_btw) \n",
    "\n",
    "    for ii in self_def_commus[tar_idx] : \n",
    "        dd_list.append(ii) \n",
    "    for ii in self_def_commus[i] : \n",
    "        dd_list.append(ii) \n",
    "    for ii in tmp_pack : \n",
    "        dd_list.append(ii) \n",
    "    dd_G = G.subgraph(dd_list) \n",
    "\n",
    "    for i in range(len(tmp_top_btw)) : \n",
    "        try : \n",
    "            print(\">>>LinkingPath \" + str(i+1) + \" : HEAD --> TAIL\\n\\t\\t\" , nx.shortest_path(dd_G , \"4932.YMR190C\" , tmp_top_btw[i]) ) \n",
    "            #break \n",
    "        except nx.NetworkXNoPath : \n",
    "            print(\"can't reach\" , \"[\" + tmp_top_btw[i] + \"]\") \n",
    "            if i == len(tmp_top_btw) - 1 : \n",
    "                print(\"[ NO PATH ]\") \n",
    "            #print(\"/////////////////////////////////////////\\n////////////// NO PATH /////////////////\\n///////////////////////////////////\") \n",
    "            \n",
    "        \n",
    "    #print(\">>>LinkingPath : HEAD --> Drifter\\n\\t\\t\" , nx.shortest_path(G , \"4932.YMR190C\" , \"4932.YDR545W\") ) \n",
    "        \n",
    "    print(\"\\n\\n\") \n",
    "\"\"\" \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############\n",
    "tmp_H(\"4932.YMR190C\" , other_sides , paddings , all_node_info, self_def_commus , G) \n",
    "########################\n",
    "\n",
    "\n",
    "#################################################################### \n",
    "#################################\n",
    "######################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison with other algorithms \n",
    "def similarity_of_clusterings(clusters1,name1, clusters2,name2):\n",
    "    clusters1 = sorted(clusters1,key = len, reverse = True)\n",
    "    clusters2 = sorted(clusters2,key = len, reverse = True)\n",
    "\n",
    "    sim_list = []\n",
    "    for i in range(0,len(clusters1)) :\n",
    "        similarity = []\n",
    "        for j in range(0,len(clusters2)):\n",
    "            counter = 0\n",
    "            for k in clusters1[i]:\n",
    "                if k in clusters2[j]:\n",
    "                    counter +=1\n",
    "            score = 2*counter/(len(clusters1[i])+len(clusters2[j]))\n",
    "            if score > 0:\n",
    "                similarity.append((str(name2)+str(j)+' '+str(len(clusters2[j]))+':',score))\n",
    "        similarity = sorted(similarity, key = lambda x:x[1], reverse= True)\n",
    "        similarity.insert(0,str(str(name1)+str(i)+' '+str(len(clusters1[i]))+':'))\n",
    "        sim_list.append(similarity)\n",
    "    return sim_list\n",
    "\n",
    "sim_llp_lmc = similarity_of_clusterings(large_comm(label_prop_1,30),'lp',large_comm(clusters,30),'mc')\n",
    "for i in sim_llp_lmc:\n",
    "    print(i[0],i[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Algorithms \n",
    "\n",
    "\n",
    "\n",
    "    # VERIFICATION STEPS \n",
    "    # Check if there are singletons: WHAT TO DO WITH THESE SINGLETONS? \n",
    "    # Check ????? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # RELEVANT OUTPUTS \n",
    "\n",
    "    # nodeassignment = \n",
    "    # Label prop and markov clustering have their output as list of list/tuples where \n",
    "    # the first layer is the list of communities and second layer is the list/tuple of the nodes within each community\n",
    "\n",
    "    # nodenames  = list(nxgraph.nodes()) #NAMES VECTOR \n",
    "    # commnum = max(value for _, value in enumerate(nodeassignment))+1 #THIS SHOULD WORK IF NODEASSIGNMENT WORKS \n",
    "    # commdict = {} #A DICTIONARY WHOSE KEY IS THE COMMUNITY AND VALUE WILL BE A LIST  OF CLUSTERS IN THAT DICT  \n",
    "\n",
    "\n",
    "    # for i in range(commnum): \n",
    "    # commdict[i] = []\n",
    "    # for nodeint, cluster in enumerate(nodeassignment): \n",
    "    #     nodename = nodenames[nodeint]\n",
    "    #     commdict[cluster].append(nodename) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # return nodenames, nodeassignment, commdict, commnum\n",
    "\n",
    "\n",
    "\n",
    "# def overlapping_algo(PARAMS HERE) #THIS IS OUR MAIN ALGORITHM  \n",
    "    \n",
    "    \n",
    "    # ANCHORS \n",
    "    # Run an initial community finding trial\n",
    "    # Determine anchors \n",
    "\n",
    "\n",
    "    # DRIFTERS\n",
    "    # SGS1connections = [\"YMR190C\", \"YNL088W\", \"YLR234W\", \"YPL024W\", \"YMR167W\"] # These are the important connections that we know SGS1 connects to \n",
    "    # testings = gp.profile( organism=\"scerevisiae\", query=[\"YMR190C\", \"YNL088W\", \"YLR234W\", \"YPL024W\", \"YMR167W\"])\n",
    "\n",
    "\n",
    "    # PATH CONSTRUCTION \n",
    "    # Use profilers to eliminate irrelevant ones\n",
    "    # Calculate ratio and define a ratio as \"good\" such that we consider it an appropriate path to go down\n",
    "    # Determine the path to go down based on ratio which can be considered as edges of a graph\n",
    "    # Repeat Step 5 to continue constructing edges\n",
    "    # If a previously visited community/\"drifter\" or a dead end is reached, choose another path with good ratio to go down or backtrack to previous nodes with appropriate paths\n",
    "    # Explore all appropriate paths???? \n",
    "\n",
    "    return #A FINAL LIST OF THE NODES THAT DENOTE A PATH STARTING FROM SGS1 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def visualization_graph() \n",
    "    # Output the resulting graph: need to get a list? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodological Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### Visualization of Graphs \n",
    "\n",
    "## PUT THE VISUALIZATION OUTPUTS HERE "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
